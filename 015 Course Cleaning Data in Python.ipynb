{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cb314-0c71-49d0-8158-36458e283235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd690dd2-4270-43ec-b903-1b261b8a268f",
   "metadata": {},
   "source": [
    "# Chapter 1: Common Data Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd119a-6af0-4dec-b4a5-3b71cf85a2f1",
   "metadata": {},
   "source": [
    "## Data type contstraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d2ad4-dc09-44c0-a670-efea7ce3bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_sharing = pd.read_csv('ride_sharing_new.csv')\n",
    "ride_sharing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780163c-0e03-4d24-965e-a98a116af231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the type of each column\n",
    "ride_sharing.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fec79b-f7d8-40de-b569-834df58888a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab multiple info (class, total entries, missing values, columns etc)\n",
    "ride_sharing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02d0a0-5fdd-4b11-bb12-180ff9ab230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab summary statistics of all numeric columns but some variables are not really numerical\n",
    "ride_sharing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3250426-f92a-4ff8-92cb-10ba5d748b95",
   "metadata": {},
   "source": [
    "### Convert int to category datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e5678-3d6a-4b54-a848-8359704a3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before converstion, the datatype is int\n",
    "ride_sharing['user_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cff5d6-ae9e-4a2a-bb9f-be0c38f381bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_type is not a numerical variable, summary statistics here are misleading. Convert the type to category.\n",
    "ride_sharing['user_type'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cc53f-e1d9-4144-bbfa-23a93fee1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column from int to category datatype.\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc29be-eb34-4a7d-a535-b248360c590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmation: assertion error if condition not satisfied\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f670db-e845-43e5-abf6-0e9f42955a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmation\n",
    "ride_sharing['user_type_cat'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b06a5-3b97-4c54-9588-8ca6c426badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes more sense now as compared to before\n",
    "ride_sharing['user_type_cat'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228af43-0433-496b-946b-e6c80803cf05",
   "metadata": {},
   "source": [
    "### Strip and then convert str to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4456bc-3475-46a5-bb2d-da0f3e22551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dtype is object/string.\n",
    "ride_sharing['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67adcd-9e9a-4d59-9792-1926116286bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, strip minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "ride_sharing['duration_trim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a837564-9856-4c16-a1b8-590f900122a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert str to int\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "ride_sharing['duration_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c22f5-449d-4ea9-bfb7-54c784f79a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm through assertation\n",
    "assert ride_sharing['duration_time'].dtype == 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27be03-3521-4709-a714-1519cb0eaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second confirmation\n",
    "ride_sharing['duration_time'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2305f6c-a571-4095-90a9-dbaead03f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_sharing[['duration', 'duration_trim', 'duration_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5eba2-fd25-4138-b940-8754f7c244f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this statistic is correct after conversion\n",
    "ride_sharing['duration_time'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06a423-8817-44d7-8fc2-2ae0918c9a97",
   "metadata": {},
   "source": [
    "## Data range constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b819727-5ff6-41fc-b15b-261d1fb26eb4",
   "metadata": {},
   "source": [
    "* Variable, tire_sizes, is unavailable in my data frame. But all slides in this section fully understood. Also, the code copied below from exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5ebca-4752-4575-82e4-8110764aa1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986986d-9137-4a78-aef5-f6b7c171117f",
   "metadata": {},
   "source": [
    "* Variable, ride_date, is unavailable in my data frame. But all slides in this section fully understood. Also, the code copied below from exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a180e51-91f2-4fda-99c4-c4d4ac3dacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to date: pd.to_date coverts ride_date column to datetime object, which is then converted to date through \n",
    "# dt.date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16616032-5b38-4c71-9712-b93a7b32d701",
   "metadata": {},
   "source": [
    "## Uniqueness constraints/Handling duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2392a-ef68-433f-aa68-6bcaf16613e4",
   "metadata": {},
   "source": [
    "* Variable, ride_id, is unavailable in my data frame. But all slides in this section fully understood. Also, the code copied below from exercises.\n",
    "* Note: duplicated() with no arguments return non-first complete duplicates (where all columns match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a19872-71ea-4ae2-8393-6f1f368f4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False\n",
    "duplicates = ride_sharing.duplicated(subset='ride_id', keep = False)\n",
    "\n",
    "# Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values(by = 'ride_id')\n",
    "\n",
    "# Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4a116-4456-45d0-875f-45908c91ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates in ride_sharing and store the results in ride_dup. drop_duplicates() drops complete duplicates.\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "print(ride_dup)\n",
    "\n",
    "# Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics. This method removes duplicate rows\n",
    "# after performing aggregate statistics, so index is reset to avoid gaps in it.\n",
    "ride_unique = ride_dup.groupby(by = 'ride_id').agg(statistics).reset_index()\n",
    "print(ride_unique)\n",
    "\n",
    "# Find duplicates again and run the assert statement to verify de-duplication\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "print(duplicates)                                                                           # contains all False now\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "print(duplicated_rides)                                                                     # returns empty data frame\n",
    "print(duplicated_rides.shape)\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0                                # the empty data frame has columns but zero rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af368f0d-4e41-4549-9f3b-7c07fb775259",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205fa4d-e50e-4418-9f32-20e589b5d5fa",
   "metadata": {},
   "source": [
    "## Membership constraints\n",
    "* Required data are unavailable. Understood slides. Code copied from the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f2559-685e-48e7-94e2-7f015ad3a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the categories DataFrame and take a close look at all possible correct categories of the survey columns\n",
    "print(categories)\n",
    "\n",
    "# Print the unique values of the survey columns in airlines using the .unique() method\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89b073-761c-44f6-9f5c-70e22620c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference \n",
    "# in the cleanliness column of categories. set(airlines['cleanliness']) creates a set of unique values in this column. Then, the\n",
    "# difference() method returns those values in the set which do not exist in cleanliness column in categories data frame (left anti\n",
    "# join).\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "print(cat_clean)\n",
    "\n",
    "# Find rows of airlines with a cleanliness value not in categories and print the output. Returns a boolean series of True/False\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "print(cat_clean_rows)\n",
    "\n",
    "# Print rows with inconsistent category. Prints all the rows for True cases.\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only. Due to ~, all the True cases becomes False, and False cases becomes True and then \n",
    "# all the rows for True cases are printed.\n",
    "print(airlines[~cat_clean_rows])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26302cc2-a1cd-4792-8cb4-74a72c6a38c9",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "* Required data are available. Exercises are solved below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e18604-a375-46e5-8c94-ceea77569193",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = pd.read_csv('airlines_final.csv')\n",
    "airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca3bf0-59de-4446-bad9-ca749ced854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values in dest_region and dest_size respectively\n",
    "airlines['dest_region'].unique(), airlines['dest_size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068095fe-1877-45cd-a554-6ead77292cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the capitalization of all values of dest_region to lowercase\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec4a3d-d784-4a96-9e4d-497f8aade2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the 'eur' with 'europe' in dest_region using the .replace() method\n",
    "mapping = {'eur':'europe'}\n",
    "airlines['dest_region'] = airlines['dest_region'].replace(mapping)\n",
    "airlines['dest_region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50c4a0-f64d-4e18-9a09-e092aec7de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip white spaces from the dest_size column using the .strip() method. With no argument given, strip removes all leading and trailing spaces.\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "airlines['dest_size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c233b09-6b48-4a2e-bc35-9479b89a1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ranges and labels for the wait_type column mentioned in the description\n",
    "label_ranges = [0, 60, 180, np.inf]                                                # create an integer list of cut-off values\n",
    "label_names = ['short', 'medium', 'long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2f805-d647-4f49-bcbc-cbb76bd063d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, labels = label_names)\n",
    "print(airlines[['wait_type', 'wait_min']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69825bf5-3ed7-4772-9b5b-c2d1ab4c9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'\n",
    "mapping = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday':'weekday', 'Thursday':'weekday', 'Friday':'weekday', 'Saturday':'weekend', \\\n",
    "          'Sunday':'weekend'}\n",
    "\n",
    "# Create the day_week column by using .replace()\n",
    "airlines['day_week'] = airlines['day'].replace(mapping)\n",
    "\n",
    "print(airlines[['day_week', 'day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff201779-b4e2-4311-ac76-23ce8c4f400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines['day'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d780537-5966-41ec-8113-8c5d6ce09599",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "* Required data are unavailable. Understood slides. Code copied from the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3baeb4-8a9a-4dc7-b7e7-b338ecfdfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"Dr.\", \"Mr.\", \"Miss\" and \"Ms.\" from full_name by replacing them with an empty string \"\" in that order\n",
    "\n",
    "airlines['full_name'] = airlines['full_name'].str.replace('Dr.',\"\")\n",
    "\n",
    "airlines['full_name'] = airlines['full_name'].str.replace('Mr.', \"\")\n",
    "\n",
    "airlines['full_name'] = airlines['full_name'].str.replace('Miss', \"\")\n",
    "\n",
    "airlines['full_name'] = airlines['full_name'].str.replace('Ms.', \"\")\n",
    "\n",
    "# Run the assert statement using .str.contains() that tests whether full_name still contains any of the honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
    "print(airlines.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be327581-5251-4e00-a7b1-f3f765164632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the airlines DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len()\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "print(resp_length)\n",
    "\n",
    "# Isolate the rows of airlines with resp_length higher than 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "print(airlines_survey)\n",
    "\n",
    "# Assert that the smallest survey_response length in airlines_survey is now bigger than 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2d438-a6ff-4210-89eb-1c132d8bc3c4",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740162d6-5868-4394-8549-23a01b04f90f",
   "metadata": {},
   "source": [
    "## Uniformity in units\n",
    "* Required data are unavailable. Understood slides. Code copied from the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c795e-44d9-4170-8edb-8c402b235749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the rows of acct_cur in banking that are equal to 'euro' and store them in the variable acct_eu\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "print(acct_eu)\n",
    "\n",
    "# Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f2f1c-7b6d-46cf-a947-31174e3c70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# onvert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return\n",
    "# a missing value\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0416fa-9e17-40b1-9c26-294e248543e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the amended account_opened column and assign it to the acct_year column\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print the newly created acct_year column\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8fe42c-de56-4c61-a9d7-b16dc741f35e",
   "metadata": {},
   "source": [
    "## Cross field validation\n",
    "* Age variable is not correct. For instance, first person should be 60 years old and not 58. Same problem with all other age entries as well. Understood slides. Code copied from the exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5bf02-7bd6-4c98-85e9-e3c482226f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "print(fund_columns)\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']\n",
    "print(inv_equ)\n",
    "\n",
    "# Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Show the number of rows in inconsistent_inv\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40862bdc-7661-4292-bad8-267dec028114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date into today, and manually calculate customers' ages and store them in ages_manual\n",
    "today = dt.date.today()\n",
    "print(today.year)\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "print(ages_manual)\n",
    "\n",
    "# Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages\n",
    "age_equ = banking['age'] == ages_manual\n",
    "print(age_equ)\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Show the number of rows in inconsistent_inv\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])\n",
    "\n",
    "    \n",
    "# NOTE: EXTRA INFO: convert 'birth_date' column from object/string to datetime first.\n",
    "banking['birth_date'] = pd.to_datetime(banking['birth_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a5e85-486c-424d-8683-6b575563922b",
   "metadata": {},
   "source": [
    "## Completeness (Missing data)\n",
    "* The required data are unavailable. Slides understood. Code copied from exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86d55b-0656-4415-92f4-32e87bd34a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of missing values by column in the banking DataFrame\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Plot and show the missingness matrix of banking with the msno.matrix() function\n",
    "msno.matrix(banking)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46cc79-9141-40e8-b38d-c3880d259feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the values of banking missing values of inv_amount into missing_investors and with non-missing inv_amount values into investors\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e3954-7bc8-4ef4-b29e-42d7e0d9d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you've isolated banking into investors and missing_investors, use the .describe() method on both of these DataFrames in the IPython\n",
    "# shell to understand whether there are structural differences between them. What do you think is going on?\n",
    "# Answer: The inv_amount is missing only for young customers, since the average age in missing_investors is 22 and the maximum age is 25\n",
    "investors.describe()\n",
    "missing_investors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbcde9-3562-4afe-b5d7-11c7c6125dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the banking DataFrame by the age column and plot the missingness matrix of banking_sorted\n",
    "banking_sorted = banking.sort_values(by = 'age')\n",
    "print(banking_sorted)\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947fc42-2a47-4319-a4f8-830dcfd44f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, you're working with another version of the banking DataFrame that contains missing values for both the cust_id column and the \n",
    "# acct_amount column. You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You \n",
    "# know that rows with missing cust_id don't really help you, and that on average acct_amount is usually 5 times the amount of inv_amount. \n",
    "# In this exercise, you will drop rows of banking with missing cust_ids, and impute missing values of acct_amount with some domain knowledge.\n",
    "\n",
    "# Use .dropna() to drop missing values of the cust_id column in banking and store the results in banking_fullid\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "print(banking_fullid)\n",
    "\n",
    "# Use inv_amount to compute the estimated account amounts for banking_fullid by setting the amounts equal to inv_amount * 5, and assign the results\n",
    "# to acct_imp\n",
    "acct_imp = banking_fullid['inv_amount'] * 5\n",
    "print(acct_imp)\n",
    "\n",
    "# Impute the missing values of acct_amount in banking_fullid with the newly created acct_imp using .fillna()\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "print(banking_imputed)\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9145e-394c-4eee-8c1b-21c077e9cc22",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12620030-0739-42e9-99ab-5bad658a7367",
   "metadata": {},
   "source": [
    "## Comparing strings\n",
    "* I have not checked availability of the data. Slides understood. Code from the exercies copied below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db3b16-eb6b-4649-a184-950a9ee8ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import process from thefuzz\n",
    "from thefuzz import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b58aed-5f62-4342-8069-aba027c52adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Return all of the unique values in the cuisine_type column of restaurants\n",
    "print(restaurants['cuisine_type'].unique())\n",
    "\n",
    "# Okay! Looks like you will need to use some string matching to correct these misspellings!\n",
    "# As a first step, create a list of all possible matches, comparing 'italian' with the restaurant types listed in the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=restaurants.shape[0])\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])\n",
    "\n",
    "# Now you're getting somewhere! Now you can iterate through matches to reassign similar entries\n",
    "\n",
    "# Within the for loop, use an if statement to check whether the similarity score in each match is greater than or equal to 80\n",
    "# If it is, use .loc to select rows where cuisine_type in restaurants is equal to the current match (which is the first element of match), and \n",
    "# reassign them to be 'italian'\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to   the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type']==match[0], 'cuisine_type']='italian'\n",
    "    \n",
    "# Finally, you'll adapt your code to work with every restaurant type in categories. Categories = ['italian', 'asian', 'american']\n",
    "# Using the variable cuisine to iterate through categories, embed your code from the previous step in an outer for loop\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=restaurants.shape[0])\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "\n",
    "# Inspect the final result      \n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f492a44-14bd-4538-917c-3933f923fde7",
   "metadata": {},
   "source": [
    "## Generating pairs\n",
    "* I have not checked availability of the data. Slides understood. Code from the exercies copied below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee269c-385f-45aa-9716-3b22638b7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the last lesson, you cleaned the restaurants dataset to make it ready for building a restaurants recommendation engine. You have a new \n",
    "# DataFrame named restaurants_new with new restaurants to train your model on, that's been scraped from a new data source. You've already cleaned \n",
    "# the cuisine_type and city columns using the techniques learned throughout the course. However you saw duplicates with typos in restaurants names \n",
    "# that require record linkage instead of joins with restaurants. In this exercise, you will perform the first step in record linkage and generate \n",
    "# possible pairs of rows between restaurants and restaurants_new. Both DataFrames, pandas and recordlinkage are in your environment.\n",
    "\n",
    "# Instantiate an indexing object by using the Index() function from recordlinkage\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block your pairing on cuisine_type by using indexer's' .block() method\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs by indexing restaurants and restaurants_new in that order\n",
    "pairs = indexer.index(restaurants, restaurants_new)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93ef2f-f0d7-48fa-b6de-7fc914b36611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the last exercise, you generated pairs between restaurants and restaurants_new in an effort to cleanly merge both DataFrames using record \n",
    "# linkage.\n",
    "# When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including \n",
    "# exact matches, string similarities, and more.\n",
    "\n",
    "# Instantiate a comparison object using the recordlinkage.Compare() function.\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DataFrames. \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')\n",
    "\n",
    "# Use the appropriate comp_cl method to find similar strings with a 0.8 similarity threshold in the rest_name column of both DataFrames.\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n",
    "# Compute the comparison of the pairs by using the .compute() method of comp_cl.\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98323652-270a-4836-a4f8-21232b61c5bc",
   "metadata": {},
   "source": [
    "## Linking DataFrames\n",
    "* I have not checked availability of the data. Slides understood. Code from the exercies copied below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558e8a1-45f6-469f-bad8-790afaedfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the last lesson, you've finished the bulk of the work on your effort to link restaurants and restaurants_new. You've generated the different \n",
    "# pairs of potentially matching rows, searched for exact matches between the cuisine_type and city columns, but compared for similar strings in the\n",
    "# rest_name column. You stored the DataFrame containing the scores in potential_matches.\n",
    "# Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of restaurants_new that are matching across the\n",
    "# columns mentioned above from potential_matches. Then you will subset restaurants_new on these indices, then append the non-duplicate values to \n",
    "# restaurants. All DataFrames are in your environment, alongside pandas imported as pd.\n",
    "\n",
    "# Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]\n",
    "print(matches)\n",
    "\n",
    "# Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the \n",
    "# .get_level_values() method.\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "print(matching_indices)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values. Or, Subset restaurants_new for rows that are not in matching_indices.\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "print(non_dup)\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16f6e6-97a8-4c4b-906b-e51f85981e4e",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739960dd-a486-421e-9031-7246650ee1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
